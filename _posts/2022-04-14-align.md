---
title: ALIGN
tags: multi-modal
author: lsh
---

(ICML 21) Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision
![image](https://1.bp.blogspot.com/-95CxjbAC6nM/YJqTiXqr7AI/AAAAAAAAHlg/iG3kb9mxck8o86epEJHkUF7V9v5sc3SdgCLcBGAsYHQ/w640-h334/image5.png)
<!--more-->

---
 
## Abstract

Conceptual Captions, MSCOCO 그리고 CLIP을 학습하는 데 사용한 데이터셋들은 non-trival data collection과 cleaning process를 가지고 있습니다. 이러한 값비싼 데이터셋 Curation 과정은 데이터셋 크기를 제한하고 학습된 모델의 크기가 커지는 것을 방해합니다. 본 논문에서 제안하는 **Dual-encoder** 구조는 contrastive loss로 정렬된 visual과 language 사이의 representation을 배웁니다. **noise한 텍스트 데이터의 corpus를 증가시키는 간단한 학습 전략**은 복잡한 텍스트, 텍스트+이미지 쿼리의 cross-modality search를 가능하게 합니다.  

## Introduction

### Why do we need noisy data?
Conceptual Captions, Visual Genome Dense Captions 그리고 ImageBERT는 human annotation, semantic parsing, cleaning and balancing 작업이 들어가기 때문에 데이터셋을 만드는데 많은 노력이 들어갑니다. 데이터 셋 역시 10M 예시 정도 밖에 만들 수 없는 단점이 있습니다. 이는 NLP pre-training에 사용되는 데이터셋의 개수에 비하면 매우 작습니다. 

### Noisy image alt-text pairs
따라서 vision-language representation learning에 필요한 데이터 개수를 증가시키기 위하여, one billion noisy image alt-text pairs를 leverage, 사용합니다. 다양한 데이터 클리닝 전략을 사용하는 것 대신에 Conceptual Captions Dataset에 오직 frequency-based filtering 전략만을 사용합니다. 이러한 exascale dataset으로 사전 학습한 모델은 다양한 task에서 강력한 성능을 가집니다. 

### Contribution
Dual-encoder 구조로 visual과 language representation이 Shared latent embedding space를 가지도록 합니다. **ALIGN: A Large-scale ImaGe and Noisy-embedding**















